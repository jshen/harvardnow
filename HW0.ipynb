{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jshen/harvardnow/blob/master/HW0.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "N9THmdC4hKWz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HW 0 - Preliminaries"
      ]
    },
    {
      "metadata": {
        "id": "WmSsl_Wnhdwl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is a mathematical component and a programming component to this homework. Please submit a PDF export of this notebook Canvas. If a question requires you to make any plots, please include those in the writeup.\n",
        "\n",
        "This assignment is intended to ensure that you have the background required for CS281, and have studied the mathematical review notes provided in section. You should be able to answer the problems below _without_ complicated calculations."
      ]
    },
    {
      "metadata": {
        "id": "9pVcTOlYw-Lx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize notebook.\n",
        "!pip install -qU plotly torch\n",
        "!rm -fr start; git clone --single-branch -b demos2018 -q https://github.com/harvard-ml-courses/cs281-demos start; cp -f start/cs281.py cs281.py\n",
        "import cs281"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnLfctzynnDB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(Run to initialize  math commands)\n",
        "$$\\newcommand{\\E}{\\mathbb{E}}\n",
        "\\newcommand{\\var}{\\text{var}}\n",
        "\\newcommand{\\R}{\\mathbb{R}}\n",
        "\\renewcommand{\\v}[1]{\\mathbf{#1}}\n",
        "$$\n"
      ]
    },
    {
      "metadata": {
        "id": "apYdAh7PhY7l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 1\n",
        "\n",
        "Let $X$ and $Y$ be two independent random variables.\n",
        "\n",
        "*  Show that the independence of $X$ and $Y$ implies that their covariance is zero.\n",
        "\n",
        "* Zero covariance  _does not_ imply independence between two random variables. Give an example of this.\n",
        "\n",
        "* For a scalar constant $a$, show the following two properties:\n",
        "\\begin{align*}\n",
        "  \\E(X + aY) &= \\E(X) + a\\E(Y)\\\\\n",
        "  \\var(X + aY) &= \\var(X) + a^2\\var(Y)\n",
        "\\end{align*}\n"
      ]
    },
    {
      "metadata": {
        "id": "gUA-SrfLlb1C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**(Student answer)**"
      ]
    },
    {
      "metadata": {
        "id": "4jjs91vTgA1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 2 - Densities\n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "* Can a probability density function (pdf) ever take values greater than 1?\n",
        "* Let $X$ be a univariate normally distributed random variable with mean 0 and variance $1/100$. What is the pdf of $X$?\n",
        "* What is the value of this pdf at 0?\n",
        "* What is the probability that $X = 0$?\n",
        "* Explain the discrepancy.\n"
      ]
    },
    {
      "metadata": {
        "id": "2MXO4SsLlpvz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**(Student answer)**"
      ]
    },
    {
      "metadata": {
        "id": "faFn2h5Gpgzn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Problem 3 - Conditioning and Bayes' rule\n",
        "  Let $\\v \\mu \\in \\R^m$ and\n",
        "  $\\v \\Sigma, \\v \\Sigma' \\in \\R^{m \\times m}$.  Let $X$ be an\n",
        "  $m$-dimensional random vector with\n",
        "  $X \\sim \\mathcal{N}(\\v \\mu, \\v \\Sigma)$, and let $Y$ be a\n",
        "  $m$-dimensional random vector such that\n",
        "  $Y | X \\sim \\mathcal{N}(X, \\v \\Sigma')$. Derive the\n",
        "  distribution and parameters for each of the following.\n",
        "\n",
        "\n",
        "* The unconditional distribution of $Y$.\n",
        "* The joint distribution for the pair $(X,Y)$.\n",
        "\n",
        "Hints:\n",
        "* You may use without proof (but they are good advanced exercises)\n",
        "  the closure properties of multivariate normal distributions. Why is\n",
        "  it helpful to know when a distribution is normal?\n",
        "* Review Eve's and Adam's Laws, linearity properties of expectation and variance, and Law of Total Covariance.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YZNEn8PBluyd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**(Student answer)**"
      ]
    },
    {
      "metadata": {
        "id": "UrzyFwBjk80s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 4 - I can Ei-gen\n",
        "\n",
        "Let $\\v X \\in \\R^{n \\times m}$.\n",
        "    \n",
        "1) What is the relationship between the $n$ eigenvalues\n",
        "      of $\\v X \\v X^T$ and the $m$ eigenvalues of $\\v X^T \\v X$?\n",
        " \n",
        " 2) Suppose $\\v X$ is square (i.e., $n=m$) and symmetric.\n",
        "      What does this tell you about the eigenvalues of $\\v X$?\n",
        "      What are the eigenvalues of $\\v X + \\v I$, where $\\v I$ is the identity matrix?\n",
        " \n",
        " 3) Suppose $\\v X$ is square, symmetric, and invertible.\n",
        "      What are the eigenvalues of $\\v X^{-1}$?\n",
        "      \n",
        "Hints:\n",
        "\n",
        "* Make use of singular value decomposition and the properties\n",
        "   of orthogonal matrices. Show your work.\n",
        "* Review and make use of (but do not derive) the spectral theorem.\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "LRzPL_nal59m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**(Student answer)**"
      ]
    },
    {
      "metadata": {
        "id": "OvXaPcFKf2k2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 5 - Vector Calculus\n",
        "Let $\\v x, \\v y \\in \\R^m$ and $\\v A \\in \\R^{m \\times m}$. Please derive from\n",
        "elementary scalar calculus the following useful properties. Write\n",
        "your final answers in vector notation.\n",
        "\n",
        "1) What is the gradient with respect to $\\v x$ of $\\v x^T \\v y$?\n",
        "\n",
        "2) What is the gradient with respect to $\\v x$ of $\\v x^T \\v x$?\n",
        "\n",
        "3) What is the gradient with respect to $\\v x$ of $\\v x^T \\v A \\v x$?\n"
      ]
    },
    {
      "metadata": {
        "id": "O09pnUMql_L1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**(Student answer)**"
      ]
    },
    {
      "metadata": {
        "id": "FEncGSA2TJbO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 6 - KL-Divergence\n",
        "\n",
        "In class we saw that the expression for KL-Divergence was defined as \n",
        "\n",
        "$$KL(p || q) = \\E_{x \\sim p} \\left(\\log \\frac{ p(x)} {  q(x) }\\right) $$ \n",
        "\n",
        "Derive the expression for the $KL$ divergence between two univariate Gaussians.\n",
        "\n",
        "You may use without proof the entropy $H = -\\E_{x \\sim p(x)} \\log(p(x))$ of the univariate Gaussian.  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nLHiXpR5mCZi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**(Student answer)**"
      ]
    },
    {
      "metadata": {
        "id": "54Y7j_P4r_Sq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Problem 7 - Gradient Check\n",
        "  Often after finishing an analytic derivation of a gradient, you will\n",
        "  need to implement it in code.  However, there may be mistakes -\n",
        "  either in the derivation or in the implementation. This is\n",
        "  particularly the case for gradients of multivariate functions.\n",
        "\n",
        "\n",
        "  One way to check your work is to numerically estimate the gradient\n",
        "  and check it on a variety of inputs. For this problem we consider\n",
        "  the simplest case of a univariate function and its derivative.  For\n",
        "  example, consider a function $f(x): \\mathbb{R} \\to \\mathbb{R}$:\n",
        "$$\\frac{d f}{d x} = \\underset{\\epsilon \\to 0} \\lim \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2 \\epsilon}$$\n",
        "\n",
        "\n",
        "A common check is to evaluate the right-hand side for a small value of\n",
        "$\\epsilon$, and check that the result is similar to your analytic\n",
        "result.\n",
        "\n",
        "\n",
        "\n",
        "In this problem, you will implement the analytic and numerical derivatives of the function $$f(x) = \\cos(x) + x^2 + e^x$$.\n",
        "\n",
        "* Implement $f$ in Python (feel free to use whatever _numpy_ functions you need):\n"
      ]
    },
    {
      "metadata": {
        "id": "fQaUOEbfsiZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def f(x):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8XIL0gEsqfz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Analytically derive the derivative of that function and implement it in Python."
      ]
    },
    {
      "metadata": {
        "id": "3n1CWtXQspml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def grad_f(x):\n",
        "    pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "625d_rV_szAL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Now, implement a gradient check (the numerical approximation to the derivative), and by plotting, \n",
        "   show that the numerical approximation approaches the analytic as $\\epsilon \n",
        "   \\to 0$ for a few values of $x$:"
      ]
    },
    {
      "metadata": {
        "id": "m6RXFJjPseFD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from plotly.offline import iplot (if you prefer plotly)\n",
        "# import plotly.graph_objs as go\n",
        "# import matplotlib.pyplot as plt (if you prefer matplotlib)\n",
        "\n",
        "def grad_check(x, epsilon):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-CjUZkx4t-4I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 8 - Introducing 🔥\n",
        "\n",
        "Throughout this class we will be making use of the PyTorch library for automatic gradient calculation. In this problem, you will implement problem 6 using PyTorch autograd. Before doing this problem, be sure to read over the following tutorials: \n",
        "\n",
        "* [PyTorch Autodifferentiation Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
        "\n",
        "* [PyTorch Example Notebook](https://colab.research.google.com/drive/1FUqJRr8NaDmVWBS-nw8lL0s0hkxHiEpK) \n",
        "\n",
        "* [PyTorch Tensor Math](https://pytorch.org/docs/stable/torch.html#math-operations)"
      ]
    },
    {
      "metadata": {
        "id": "1AJpv5faucQO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Implement the same function as in problem 6 but now using pytorch."
      ]
    },
    {
      "metadata": {
        "id": "sabs3P1cvtz4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def f2(x):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vC_u65eQv6sQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* For several values of $x$ compute the value of $f$ and the gradient with respect to $x$ using autodifferentiation. "
      ]
    },
    {
      "metadata": {
        "id": "xOsVjxynqcL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# z = f2()\n",
        "# Use autodifferentiation to compute grad.  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mdsXx5MFx--n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Now let's consider \"adding\" your implementation of _gradf_ to PyTorch. That is, have it call your \n",
        "version to compute gradients for this function.  Implement the following interface and check that it produces the same gradients as above. "
      ]
    },
    {
      "metadata": {
        "id": "d2wvADfawWVs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class MyF(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        \n",
        "        # Implementation of f2.\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        \n",
        "        # PyTorch Implementation of grad_f\n",
        "        \n",
        "f3 = MyF.apply"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}